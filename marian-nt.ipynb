{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c793cb2-654a-4540-b486-4c4a55d64016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 10577\n",
      "{'en': 'Prime Minister Narendra Modi met Her Majesty Queen Maxima of the Kingdom of the Netherlands today.', 'ta': 'சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Paths to your files (edit this to your folder)\n",
    "# data_dir = Path(\"/../en-ta.txt/\")\n",
    "en_path = \"/Users/niteeshkumar/Documents/document_level_nmt/en-ta.txt/pmindia.en-ta.en\"\n",
    "ta_path = \"/Users/niteeshkumar/Documents/document_level_nmt/en-ta.txt/pmindia.en-ta.ta\"\n",
    "\n",
    "# 1. Read files\n",
    "with open(en_path, \"r\", encoding=\"utf-8\") as f_en:\n",
    "    en_lines = [line.strip() for line in f_en]\n",
    "\n",
    "with open(ta_path, \"r\", encoding=\"utf-8\") as f_ta:\n",
    "    ta_lines = [line.strip() for line in f_ta]\n",
    "\n",
    "assert len(en_lines) == len(ta_lines), \"Line counts do not match!\"\n",
    "\n",
    "# 2. Build parallel sentence pairs\n",
    "parallel_pairs = [\n",
    "    {\"en\": src, \"ta\": tgt}\n",
    "    for src, tgt in zip(en_lines, ta_lines)\n",
    "    if src and tgt  # drop empty lines\n",
    "]\n",
    "\n",
    "print(f\"Total pairs: {len(parallel_pairs)}\")\n",
    "print(parallel_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255a8abc-5c93-491d-a535-a05e336e67a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The rationalization of Autonomous Bodies under Department of Health & Family Welfare will involve inter-ministerial consultations and review of existing bye laws of these bodies.\\tசுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.\\n'\n",
      "'The time frame for implementation is one year,\\tஇதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.\\n'\n",
      "'Since the DoHFW provides funds to the hospitals, the grants can be given from the Department to the hospital directly.\\tமருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் குடும்ப நலத் துறை வழங்கி வருவதால், நிதியுதவியையும் மருத்துமனைகளுக்கு துறையே நேரில் வழங்க முடியும்.\\n'\n",
      "'RAN functions can, therefore, be vested in DoHFW.\\tதேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சுகாதாரம் மற்றும் குடும்பநலத் துறையிடம் இருக்கும்.\\n'\n",
      "'Managing Committee of RAN Society will meet to dissolve the Autonomous Body (AB) as per provisions of Societies Registration Act, 1860 (SRA).\\tசங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறைகளின்படி, தன்னாட்சி அமைப்பை கலைக்க, தேசிய ஆரோக்கிய நிதி சங்கத்தின் நிர்வாகிகள் குழு கூடும்.\\n'\n",
      "'In addition to this, Health Minister’s Cancer Patient Fund (HMCPF) shall also be transferred to the Department.\\tஇதற்கு மேலாக, சுகாதார அமைச்சரின் புற்றுநோயால் பாதிக்கப்பட்ட நோயாளிகளுக்கான நிதி-யும், துறைக்கே மாற்றப்படுகிறது.\\n'\n",
      "'The timeline required for this is one year.\\tஇதற்கு ஓராண்டு கால அவகாசம் தேவைப்படுகிறது.\\n'\n",
      "'Jansankhya Sthirata Kosh (JSK) was set up with a corpus grant of Rs.100 crores in the year 2003 to raise awareness for population stabilization strategies.\\tமக்கள் தொகையை நிலைப்படுத்துவதற்கான உத்திகள் குறித்து விழிப்புணர்வை ஏற்படுத்துவதற்காக, ரூ.100 கோடி நிதியுடன், கடந்த 2003-ம் ஆண்டில் தேசிய மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைக்கப்பட்டது.\\n'\n",
      "'JSK organizes various activities with target populations as a part of its mandate.\\tதேசிய மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைப்பு, தனது நோக்கத்தின் ஒரு பகுதியாக, மக்கள் தொகையை இலக்காகக் கொண்டு பல்வேறு நிகழ்ச்சிகளுக்கு ஏற்பாடு செய்தது.\\n'\n",
      "'There has been no continuous funding to JSK from the Ministry.\\tஅமைச்சகத்திடமிருந்து மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைப்புக்கு தொடர்ந்து நிதி வழங்கப்படவில்லை.\\n'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tsv_path = \"/Users/niteeshkumar/Documents/document_level_nmt/pmindia.v1.ta-en.tsv\" # update this\n",
    "\n",
    "with open(tsv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(repr(line))\n",
    "        if i == 9:   # first 10 lines\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663e224c-4a6c-444c-a477-979fc5c5aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 39526\n",
      "{'en': 'The rationalization of Autonomous Bodies under Department of Health & Family Welfare will involve inter-ministerial consultations and review of existing bye laws of these bodies.', 'ta': 'சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.'}\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "with open(tsv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 2:\n",
    "            continue  # skip any abnormal lines\n",
    "\n",
    "        en, ta = parts\n",
    "        en, ta = en.strip(), ta.strip()\n",
    "        if en and ta:\n",
    "            pairs.append({\"en\": en, \"ta\": ta})\n",
    "\n",
    "print(\"Total pairs:\", len(pairs))\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c14eb3f-9c28-449c-a783-8fbbd9e993a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9443669-4598-4c32-b35f-7f1196a914dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch\n",
    "\n",
    "# model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "# batch_size = 64\n",
    "# aligned_pairs = []\n",
    "\n",
    "# for start in range(0, len(pairs), batch_size):\n",
    "#     end = start + batch_size\n",
    "#     en_batch = [p[\"en\"] for p in pairs[start:end]]\n",
    "#     ta_batch = [p[\"ta\"] for p in pairs[start:end]]\n",
    "\n",
    "#     en_emb = model.encode(en_batch, convert_to_tensor=True)\n",
    "#     ta_emb = model.encode(ta_batch, convert_to_tensor=True)\n",
    "\n",
    "#     cos_scores = util.cos_sim(en_emb, ta_emb).diagonal()  # only batch diag\n",
    "\n",
    "#     for pair, score in zip(pairs[start:end], cos_scores):\n",
    "#         if score.item() >= 0.7:\n",
    "#             aligned_pairs.append({\n",
    "#                 \"en\": pair[\"en\"],\n",
    "#                 \"ta\": pair[\"ta\"],\n",
    "#                 \"score\": float(score),\n",
    "#             })\n",
    "\n",
    "# print(\"Original:\", len(pairs), \"Aligned:\", len(aligned_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f32eb2-5b05-45d5-bd6d-f5b9152d2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape(aligned_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea53eb79-d49f-4d60-b929-09f19ce7d62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 4941\n",
      "First doc len: 8\n"
     ]
    }
   ],
   "source": [
    "DOC_LEN = 8  # sentences per pseudo-document\n",
    "\n",
    "documents = []\n",
    "for i in range(0, len(pairs), DOC_LEN):\n",
    "    chunk = pairs[i:i+DOC_LEN]\n",
    "    if len(chunk) < 2:\n",
    "        break\n",
    "    documents.append(chunk)\n",
    "\n",
    "print(\"Num documents:\", len(documents))\n",
    "print(\"First doc len:\", len(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41405122-a37e-4259-b268-438fcc4f7537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'The rationalization of Autonomous Bodies under Department of Health & Family Welfare will involve inter-ministerial consultations and review of existing bye laws of these bodies.',\n",
       "  'ta': 'சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.'},\n",
       " {'en': 'The time frame for implementation is one year,',\n",
       "  'ta': 'இதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.'},\n",
       " {'en': 'Since the DoHFW provides funds to the hospitals, the grants can be given from the Department to the hospital directly.',\n",
       "  'ta': 'மருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் குடும்ப நலத் துறை வழங்கி வருவதால், நிதியுதவியையும் மருத்துமனைகளுக்கு துறையே நேரில் வழங்க முடியும்.'},\n",
       " {'en': 'RAN functions can, therefore, be vested in DoHFW.',\n",
       "  'ta': 'தேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சுகாதாரம் மற்றும் குடும்பநலத் துறையிடம் இருக்கும்.'},\n",
       " {'en': 'Managing Committee of RAN Society will meet to dissolve the Autonomous Body (AB) as per provisions of Societies Registration Act, 1860 (SRA).',\n",
       "  'ta': 'சங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறைகளின்படி, தன்னாட்சி அமைப்பை கலைக்க, தேசிய ஆரோக்கிய நிதி சங்கத்தின் நிர்வாகிகள் குழு கூடும்.'},\n",
       " {'en': 'In addition to this, Health Minister’s Cancer Patient Fund (HMCPF) shall also be transferred to the Department.',\n",
       "  'ta': 'இதற்கு மேலாக, சுகாதார அமைச்சரின் புற்றுநோயால் பாதிக்கப்பட்ட நோயாளிகளுக்கான நிதி-யும், துறைக்கே மாற்றப்படுகிறது.'},\n",
       " {'en': 'The timeline required for this is one year.',\n",
       "  'ta': 'இதற்கு ஓராண்டு கால அவகாசம் தேவைப்படுகிறது.'},\n",
       " {'en': 'Jansankhya Sthirata Kosh (JSK) was set up with a corpus grant of Rs.100 crores in the year 2003 to raise awareness for population stabilization strategies.',\n",
       "  'ta': 'மக்கள் தொகையை நிலைப்படுத்துவதற்கான உத்திகள் குறித்து விழிப்புணர்வை ஏற்படுத்துவதற்காக, ரூ.100 கோடி நிதியுடன், கடந்த 2003-ம் ஆண்டில் தேசிய மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைக்கப்பட்டது.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e943d5f7-2c03-42a7-b971-4326b3c65909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39526"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9b927b-3d8b-41cd-be8a-b4d8046d5e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The rationalization of Autonomous Bodies under...</td>\n",
       "      <td>சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The time frame for implementation is one year,</td>\n",
       "      <td>இதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>மருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் க...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAN functions can, therefore, be vested in DoHFW.</td>\n",
       "      <td>தேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சு...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>சங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறை...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>We will also get to see its direct benefit dur...</td>\n",
       "      <td>இரு நாடுகளையும் சேர்ந்த தொழில் துறையினருடன் இன...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39522</th>\n",
       "      <td>I am confident that our dialogue and the agree...</td>\n",
       "      <td>இன்றைய நமது பேச்சுவார்த்தைகள் மற்றும் கையெழுத்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39523</th>\n",
       "      <td>With these words, I once again warmly welcome ...</td>\n",
       "      <td>இந்தக் கருத்துகளுடன், பிரதமர் அபே-வையும், அவரு...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39524</th>\n",
       "      <td>Ijyo De Gozaimas (That’s all for now)</td>\n",
       "      <td>Ijyo De Gozaimas (தற்போது இவ்வளவு தான்)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39525</th>\n",
       "      <td>Arigato Gozaimas (Thank you)</td>\n",
       "      <td>Arigato Gozaimas (உங்களுக்கு நன்றி.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39526 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      The rationalization of Autonomous Bodies under...   \n",
       "1         The time frame for implementation is one year,   \n",
       "2      Since the DoHFW provides funds to the hospital...   \n",
       "3      RAN functions can, therefore, be vested in DoHFW.   \n",
       "4      Managing Committee of RAN Society will meet to...   \n",
       "...                                                  ...   \n",
       "39521  We will also get to see its direct benefit dur...   \n",
       "39522  I am confident that our dialogue and the agree...   \n",
       "39523  With these words, I once again warmly welcome ...   \n",
       "39524              Ijyo De Gozaimas (That’s all for now)   \n",
       "39525                       Arigato Gozaimas (Thank you)   \n",
       "\n",
       "                                                      ta  \n",
       "0      சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள...  \n",
       "1            இதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.  \n",
       "2      மருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் க...  \n",
       "3      தேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சு...  \n",
       "4      சங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறை...  \n",
       "...                                                  ...  \n",
       "39521  இரு நாடுகளையும் சேர்ந்த தொழில் துறையினருடன் இன...  \n",
       "39522  இன்றைய நமது பேச்சுவார்த்தைகள் மற்றும் கையெழுத்...  \n",
       "39523  இந்தக் கருத்துகளுடன், பிரதமர் அபே-வையும், அவரு...  \n",
       "39524            Ijyo De Gozaimas (தற்போது இவ்வளவு தான்)  \n",
       "39525               Arigato Gozaimas (உங்களுக்கு நன்றி.)  \n",
       "\n",
       "[39526 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pairs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef152d8b-4727-4cf4-8f73-b4b5d3f25cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[:1000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7597e4-475d-48cc-a1c8-97a854a973e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77047b88-16b9-4558-aff2-f6fa306edc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7500f48-6696-41cd-a2b6-3eda1ebd607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import math\n",
    "# import json\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "#                            (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66193af9-74d4-44c9-b654-26765e23196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # ---------- Positional Encoding ----------\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         pe = torch.zeros(max_len, d_model)          # [max_len, d_model]\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "#                              * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)                        # [1, max_len, d_model]\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, d_model]\n",
    "#         x = x + self.pe[:, :x.size(1), :]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "\n",
    "# # ---------- Multi-Head Attention ----------\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         assert d_model % n_heads == 0\n",
    "#         self.d_model = d_model\n",
    "#         self.n_heads = n_heads\n",
    "#         self.d_k = d_model // n_heads\n",
    "\n",
    "#         self.W_q = nn.Linear(d_model, d_model)\n",
    "#         self.W_k = nn.Linear(d_model, d_model)\n",
    "#         self.W_v = nn.Linear(d_model, d_model)\n",
    "#         self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, query, key, value, mask=None):\n",
    "#         # query/key/value: [batch, seq_len, d_model]\n",
    "#         batch_size = query.size(0)\n",
    "\n",
    "#         # Linear projections\n",
    "#         Q = self.W_q(query)  # [batch, seq, d_model]\n",
    "#         K = self.W_k(key)\n",
    "#         V = self.W_v(value)\n",
    "\n",
    "#         # Split into heads: [batch, heads, seq, d_k]\n",
    "#         def split_heads(x):\n",
    "#             return x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "#         Q = split_heads(Q)\n",
    "#         K = split_heads(K)\n",
    "#         V = split_heads(V)\n",
    "\n",
    "#         # Scaled dot-product attention\n",
    "#         scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "#         # scores: [batch, heads, seq_q, seq_k]\n",
    "\n",
    "#         if mask is not None:\n",
    "#             # mask: [batch, 1, 1, seq_k] or [batch, 1, seq_q, seq_k]\n",
    "#             scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "#         attn = F.softmax(scores, dim=-1)\n",
    "#         attn = self.dropout(attn)\n",
    "\n",
    "#         context = torch.matmul(attn, V)  # [batch, heads, seq_q, d_k]\n",
    "\n",
    "#         # Concatenate heads\n",
    "#         context = context.transpose(1, 2).contiguous().view(\n",
    "#             batch_size, -1, self.d_model\n",
    "#         )  # [batch, seq_q, d_model]\n",
    "\n",
    "#         out = self.W_o(context)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # ---------- Position-wise Feed Forward ----------\n",
    "\n",
    "# class PositionwiseFeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(d_model, d_ff)\n",
    "#         self.fc2 = nn.Linear(d_ff, d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "# # ---------- Encoder / Decoder Layers ----------\n",
    "\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "#         self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, src_mask=None):\n",
    "#         # Self-attention\n",
    "#         attn_out = self.self_attn(x, x, x, src_mask)\n",
    "#         x = self.norm1(x + self.dropout(attn_out))\n",
    "#         # Feed-forward\n",
    "#         ff_out = self.ff(x)\n",
    "#         x = self.norm2(x + self.dropout(ff_out))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "#         self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "#         self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.norm3 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "#         # Masked self-attention (decoder)\n",
    "#         self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "#         x = self.norm1(x + self.dropout(self_attn_out))\n",
    "#         # Encoder–decoder attention\n",
    "#         cross_attn_out = self.cross_attn(x, memory, memory, memory_mask)\n",
    "#         x = self.norm2(x + self.dropout(cross_attn_out))\n",
    "#         # Feed-forward\n",
    "#         ff_out = self.ff(x)\n",
    "#         x = self.norm3(x + self.dropout(ff_out))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # ---------- Encoder / Decoder Stacks ----------\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, src, src_mask=None):\n",
    "#         # src: [batch, src_len]\n",
    "#         x = self.tok_embed(src)   # [batch, src_len, d_model]\n",
    "#         x = self.pos_embed(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, src_mask)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "#         )\n",
    "#         self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "#     def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "#         # tgt: [batch, tgt_len]\n",
    "#         x = self.tok_embed(tgt)\n",
    "#         x = self.pos_embed(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, memory, tgt_mask, memory_mask)\n",
    "#         logits = self.fc_out(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # ---------- Full Transformer ----------\n",
    "\n",
    "# def generate_square_subsequent_mask(sz, device):\n",
    "#     # standard causal mask for decoder\n",
    "#     mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "#     return ~mask  # True where allowed, False where masked\n",
    "\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         src_vocab_size,\n",
    "#         tgt_vocab_size,\n",
    "#         d_model=512,\n",
    "#         N=6,\n",
    "#         n_heads=8,\n",
    "#         d_ff=2048,\n",
    "#         dropout=0.1,\n",
    "#         max_len=5000,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.encoder = Encoder(src_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "#         self.decoder = Decoder(tgt_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "\n",
    "#     def make_src_mask(self, src, pad_idx):\n",
    "#         # src: [batch, src_len]\n",
    "#         mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "#         # [batch, 1, 1, src_len]\n",
    "#         return mask\n",
    "\n",
    "#     def make_tgt_mask(self, tgt, pad_idx):\n",
    "#         # tgt: [batch, tgt_len]\n",
    "#         batch_size, tgt_len = tgt.size()\n",
    "#         pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len]\n",
    "#         subseq_mask = generate_square_subsequent_mask(tgt_len, tgt.device)  # [tgt_len, tgt_len]\n",
    "#         subseq_mask = subseq_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, tgt_len, tgt_len]\n",
    "#         mask = pad_mask & subseq_mask\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "#         src_mask = self.make_src_mask(src, src_pad_idx)\n",
    "#         tgt_mask = self.make_tgt_mask(tgt, tgt_pad_idx)\n",
    "#         memory = self.encoder(src, src_mask)\n",
    "#         out = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# # Pseudo-code: adapt to your actual file format.\n",
    "# def parse_line(line):\n",
    "#     # Example input:\n",
    "#     # \"enta0The rationalization ...சுகாதாரம் ...1\"\n",
    "#     # You likely need a known separator; if not, pre-clean in a script.\n",
    "#     # For illustration, assume tab-separated: id \\t en \\t ta \\t idx\n",
    "#     parts = line.strip().split(\"\\t\")\n",
    "#     en = parts[1]\n",
    "#     ta = parts[2]\n",
    "#     return en, ta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67e93b81-226d-4014-92c5-e4fb509e9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # After you build vocabularies:\n",
    "# src_vocab_size = len(src_vocab)\n",
    "# tgt_vocab_size = len(tgt_vocab)\n",
    "# pad_idx = 0  # make sure 0 is PAD in both vocabs\n",
    "\n",
    "# model = Transformer(\n",
    "#     src_vocab_size=src_vocab_size,\n",
    "#     tgt_vocab_size=tgt_vocab_size,\n",
    "#     d_model=512,\n",
    "#     N=6,\n",
    "#     n_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     dropout=0.1,\n",
    "#     max_len=256,\n",
    "# ).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(),\n",
    "#     lr=1e-4,\n",
    "#     betas=(0.9, 0.98),\n",
    "#     eps=1e-9,\n",
    "# )\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     for batch in train_dataloader:\n",
    "#         src, tgt = batch[\"src\"].to(device), batch[\"tgt\"].to(device)\n",
    "#         # Teacher forcing: decoder input is tgt[:, :-1], target labels are tgt[:, 1:]\n",
    "#         tgt_inp = tgt[:, :-1]\n",
    "#         tgt_out = tgt[:, 1:]\n",
    "\n",
    "#         logits = model(src, tgt_inp, pad_idx, pad_idx)  # [batch, tgt_len-1, vocab]\n",
    "#         loss = criterion(\n",
    "#             logits.reshape(-1, tgt_vocab_size),\n",
    "#             tgt_out.reshape(-1)\n",
    "#         )\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}: loss = {total_loss / len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06855ad-f8e4-4c00-bac5-8bdc836b28e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6735c-dd6f-48dc-92cf-736c2c5de148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c55e78-b629-415e-93df-bb412eb3fa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442135e1-73a8-4d61-80c2-ea9508d30a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4b402-907a-4662-be4d-04393056526a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e251a881-e648-4c9e-aea2-983b9863cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vanila transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f84701d6-09b1-4865-b04f-72d01c6d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from collections import Counter\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # 1. Vocab building & tokenization\n",
    "# # ===============================\n",
    "\n",
    "# PAD_TOKEN = \"<pad>\"\n",
    "# SOS_TOKEN = \"<sos>\"\n",
    "# EOS_TOKEN = \"<eos>\"\n",
    "# UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# def simple_tokenize(text: str):\n",
    "#     # Replace with a better tokenizer if you want.\n",
    "#     return text.strip().split()\n",
    "\n",
    "# def build_vocab(texts, min_freq=1):\n",
    "#     counter = Counter()\n",
    "#     for t in texts:\n",
    "#         counter.update(simple_tokenize(str(t)))\n",
    "#     itos = list(special_tokens)\n",
    "#     for tok, freq in counter.items():\n",
    "#         if freq >= min_freq and tok not in itos:\n",
    "#             itos.append(tok)\n",
    "#     stoi = {tok: idx for idx, tok in enumerate(itos)}\n",
    "#     return stoi, itos\n",
    "\n",
    "# # Build vocabularies from dataframe\n",
    "# src_vocab, src_itos = build_vocab(df[\"en\"].tolist(), min_freq=1)\n",
    "# tgt_vocab, tgt_itos = build_vocab(df[\"ta\"].tolist(), min_freq=1)\n",
    "\n",
    "# pad_idx = src_vocab[PAD_TOKEN]\n",
    "# assert pad_idx == tgt_vocab[PAD_TOKEN]\n",
    "\n",
    "# def encode_sentence(text, vocab, max_len=256):\n",
    "#     tokens = [SOS_TOKEN] + simple_tokenize(str(text)) + [EOS_TOKEN]\n",
    "#     ids = [vocab.get(tok, vocab[UNK_TOKEN]) for tok in tokens]\n",
    "#     if len(ids) < max_len:\n",
    "#         ids += [vocab[PAD_TOKEN]] * (max_len - len(ids))\n",
    "#     else:\n",
    "#         ids = ids[:max_len]\n",
    "#     return ids\n",
    "\n",
    "# max_len = 256\n",
    "\n",
    "# src_seqs = [encode_sentence(t, src_vocab, max_len=max_len) for t in df[\"en\"]]\n",
    "# tgt_seqs = [encode_sentence(t, tgt_vocab, max_len=max_len) for t in df[\"ta\"]]\n",
    "\n",
    "# src_tensor = torch.tensor(src_seqs, dtype=torch.long)\n",
    "# tgt_tensor = torch.tensor(tgt_seqs, dtype=torch.long)\n",
    "\n",
    "# # ===============================\n",
    "# # 2. Dataset & DataLoader\n",
    "# # ===============================\n",
    "\n",
    "# class NMTDataset(Dataset):\n",
    "#     def __init__(self, src_tensor, tgt_tensor):\n",
    "#         self.src = src_tensor\n",
    "#         self.tgt = tgt_tensor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.src.size(0)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             \"src\": self.src[idx],\n",
    "#             \"tgt\": self.tgt[idx],\n",
    "#         }\n",
    "\n",
    "# dataset = NMTDataset(src_tensor, tgt_tensor)\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=64,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "\n",
    "# # ===============================\n",
    "# # 3. Transformer components\n",
    "# # ===============================\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         pe = torch.zeros(max_len, d_model)          # [max_len, d_model]\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "#                              * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)                        # [1, max_len, d_model]\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: [batch_size, seq_len, d_model]\n",
    "#         x = x + self.pe[:, :x.size(1), :]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         assert d_model % n_heads == 0\n",
    "#         self.d_model = d_model\n",
    "#         self.n_heads = n_heads\n",
    "#         self.d_k = d_model // n_heads\n",
    "\n",
    "#         self.W_q = nn.Linear(d_model, d_model)\n",
    "#         self.W_k = nn.Linear(d_model, d_model)\n",
    "#         self.W_v = nn.Linear(d_model, d_model)\n",
    "#         self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, query, key, value, mask=None):\n",
    "#         # query/key/value: [batch, seq_len, d_model]\n",
    "#         batch_size = query.size(0)\n",
    "\n",
    "#         Q = self.W_q(query)\n",
    "#         K = self.W_k(key)\n",
    "#         V = self.W_v(value)\n",
    "\n",
    "#         def split_heads(x):\n",
    "#             return x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "#         Q = split_heads(Q)\n",
    "#         K = split_heads(K)\n",
    "#         V = split_heads(V)\n",
    "\n",
    "#         scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "#         # scores: [batch, heads, seq_q, seq_k]\n",
    "\n",
    "#         if mask is not None:\n",
    "#             # mask: [batch, 1, 1, seq_k] or [batch, 1, seq_q, seq_k]\n",
    "#             scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "#         attn = F.softmax(scores, dim=-1)\n",
    "#         attn = self.dropout(attn)\n",
    "\n",
    "#         context = torch.matmul(attn, V)  # [batch, heads, seq_q, d_k]\n",
    "\n",
    "#         context = context.transpose(1, 2).contiguous().view(\n",
    "#             batch_size, -1, self.d_model\n",
    "#         )  # [batch, seq_q, d_model]\n",
    "\n",
    "#         out = self.W_o(context)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class PositionwiseFeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(d_model, d_ff)\n",
    "#         self.fc2 = nn.Linear(d_ff, d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "#         self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, src_mask=None):\n",
    "#         attn_out = self.self_attn(x, x, x, src_mask)\n",
    "#         x = self.norm1(x + self.dropout(attn_out))\n",
    "#         ff_out = self.ff(x)\n",
    "#         x = self.norm2(x + self.dropout(ff_out))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "#         self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "#         self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.norm3 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "#         self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "#         x = self.norm1(x + self.dropout(self_attn_out))\n",
    "#         cross_attn_out = self.cross_attn(x, memory, memory, memory_mask)\n",
    "#         x = self.norm2(x + self.dropout(cross_attn_out))\n",
    "#         ff_out = self.ff(x)\n",
    "#         x = self.norm3(x + self.dropout(ff_out))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, src, src_mask=None):\n",
    "#         x = self.tok_embed(src)   # [batch, src_len, d_model]\n",
    "#         x = self.pos_embed(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, src_mask)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "#         self.layers = nn.ModuleList(\n",
    "#             [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "#         )\n",
    "#         self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "#     def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "#         x = self.tok_embed(tgt)\n",
    "#         x = self.pos_embed(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, memory, tgt_mask, memory_mask)\n",
    "#         logits = self.fc_out(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# def generate_square_subsequent_mask(sz, device):\n",
    "#     mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "#     return ~mask  # True where allowed, False where masked\n",
    "\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         src_vocab_size,\n",
    "#         tgt_vocab_size,\n",
    "#         d_model=512,\n",
    "#         N=6,\n",
    "#         n_heads=8,\n",
    "#         d_ff=2048,\n",
    "#         dropout=0.1,\n",
    "#         max_len=5000,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.encoder = Encoder(src_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "#         self.decoder = Decoder(tgt_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "\n",
    "#     def make_src_mask(self, src, pad_idx):\n",
    "#         mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "#         return mask\n",
    "\n",
    "#     def make_tgt_mask(self, tgt, pad_idx):\n",
    "#         batch_size, tgt_len = tgt.size()\n",
    "#         pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len]\n",
    "#         subseq_mask = generate_square_subsequent_mask(tgt_len, tgt.device)  # [tgt_len, tgt_len]\n",
    "#         subseq_mask = subseq_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, tgt_len, tgt_len]\n",
    "#         mask = pad_mask & subseq_mask\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "#         src_mask = self.make_src_mask(src, src_pad_idx)\n",
    "#         tgt_mask = self.make_tgt_mask(tgt, tgt_pad_idx)\n",
    "#         memory = self.encoder(src, src_mask)\n",
    "#         out = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
    "#         return out\n",
    "\n",
    "# # ===============================\n",
    "# # 4. Training setup\n",
    "# # ===============================\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# src_vocab_size = len(src_vocab)\n",
    "# tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "# model = Transformer(\n",
    "#     src_vocab_size=src_vocab_size,\n",
    "#     tgt_vocab_size=tgt_vocab_size,\n",
    "#     d_model=512,\n",
    "#     N=6,\n",
    "#     n_heads=8,\n",
    "#     d_ff=2048,\n",
    "#     dropout=0.1,\n",
    "#     max_len=max_len,\n",
    "# ).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(),\n",
    "#     lr=1e-4,\n",
    "#     betas=(0.9, 0.98),\n",
    "#     eps=1e-9,\n",
    "# )\n",
    "\n",
    "# # ===============================\n",
    "# # 5. Training loop\n",
    "# # ===============================\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for batch in train_dataloader:\n",
    "#         src = batch[\"src\"].to(device)\n",
    "#         tgt = batch[\"tgt\"].to(device)\n",
    "\n",
    "#         # Teacher forcing: decoder input is tgt[:, :-1], labels are tgt[:, 1:]\n",
    "#         tgt_inp = tgt[:, :-1]\n",
    "#         tgt_out = tgt[:, 1:]\n",
    "\n",
    "#         logits = model(src, tgt_inp, pad_idx, pad_idx)  # [batch, tgt_len-1, vocab]\n",
    "\n",
    "#         loss = criterion(\n",
    "#             logits.reshape(-1, tgt_vocab_size),\n",
    "#             tgt_out.reshape(-1),\n",
    "#         )\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_dataloader)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c7c3c4c-03f4-4a75-befe-01825795023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "\n",
    "# # =========================\n",
    "# # 0. Basic setup\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # df = pd.read_csv(\"pmindia_en_ta_39k.csv\")  # ensure df['en'], df['ta'] exist\n",
    "\n",
    "# # Optional: shuffle / small train–val split\n",
    "# # df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "# # val_frac = 0.05\n",
    "# # val_size = int(len(df) * val_frac)\n",
    "# # df_train = df.iloc[:-val_size]\n",
    "# # df_val = df.iloc[-val_size:]\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # 1. Load Marian en→hi model\n",
    "# # =========================\n",
    "# model_name = \"Helsinki-NLP/opus-mt-en-hi\"   # pretrained English→Indic\n",
    "# tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "# model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # 2. Dataset class\n",
    "# # =========================\n",
    "# class EnTaMarianDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer, max_length=128):\n",
    "#         self.en = df[\"en\"].astype(str).tolist()\n",
    "#         self.ta = df[\"ta\"].astype(str).tolist()\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.en)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         src_text = self.en[idx]\n",
    "#         tgt_text = self.ta[idx]\n",
    "\n",
    "#         # Source: English\n",
    "#         src_enc = self.tokenizer(\n",
    "#             src_text,\n",
    "#             max_length=self.max_length,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "\n",
    "#         # Target: Tamil\n",
    "#         with self.tokenizer.as_target_tokenizer():\n",
    "#             tgt_enc = self.tokenizer(\n",
    "#                 tgt_text,\n",
    "#                 max_length=self.max_length,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#                 return_tensors=\"pt\",\n",
    "#             )\n",
    "\n",
    "#         input_ids = src_enc[\"input_ids\"].squeeze(0)\n",
    "#         attention_mask = src_enc[\"attention_mask\"].squeeze(0)\n",
    "#         labels = tgt_enc[\"input_ids\"].squeeze(0)\n",
    "\n",
    "#         # Mask pad tokens in labels with -100 so they are ignored by CE loss\n",
    "#         labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"attention_mask\": attention_mask,\n",
    "#             \"labels\": labels,\n",
    "#         }\n",
    "\n",
    "\n",
    "# train_dataset = EnTaMarianDataset(df_train, tokenizer, max_length=128)\n",
    "# val_dataset = EnTaMarianDataset(df_val, tokenizer, max_length=128)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # 3. Optimizer\n",
    "# # =========================\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # 4. Training + simple validation\n",
    "# # =========================\n",
    "# num_epochs = 3\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # ---- train ----\n",
    "#     model.train()\n",
    "#     total_train_loss = 0.0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             labels=labels,\n",
    "#         )\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_train_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "#     # ---- validation (optional) ----\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#             outputs = model(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 labels=labels,\n",
    "#             )\n",
    "#             loss = outputs.loss\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} | train_loss={avg_train_loss:.4f} | val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # 5. Save fine-tuned model\n",
    "# # =========================\n",
    "# model.save_pretrained(\"marian_enhi_to_entamil_finetuned\")\n",
    "# tokenizer.save_pretrained(\"marian_enhi_to_entamil_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2b1f3-9990-46af-b0db-bb74520d26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Marian model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7547c-d725-4eb4-9347-675d2f04e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# df = pd.read_csv(\"pmindia_en_ta_39k.csv\")  # df['en'], df['ta']\n",
    "\n",
    "# =========================\n",
    "# 1. Load Marian en→hi model\n",
    "# =========================\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"   # pretrained English→Indic\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset class (full 39k)\n",
    "# =========================\n",
    "class EnTaMarianDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.en = df[\"en\"].astype(str).tolist()\n",
    "        self.ta = df[\"ta\"].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.en[idx]\n",
    "        tgt_text = self.ta[idx]\n",
    "\n",
    "        # Source: English\n",
    "        src_enc = self.tokenizer(\n",
    "            src_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Target: Tamil\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tgt_enc = self.tokenizer(\n",
    "                tgt_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        input_ids = src_enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = src_enc[\"attention_mask\"].squeeze(0)\n",
    "        labels = tgt_enc[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        # Mask pad tokens in labels with -100 so they are ignored by CE loss\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "dataset = EnTaMarianDataset(df, tokenizer, max_length=128)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Optimizer\n",
    "# =========================\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Training loop on full 39k\n",
    "# =========================\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | train_loss={avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. Save fine-tuned model\n",
    "# =========================\n",
    "model.save_pretrained(\"marian_enhi_to_entamil_finetuned_full39k\")\n",
    "tokenizer.save_pretrained(\"marian_enhi_to_entamil_finetuned_full39k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c50b0-bc42-4b51-85bd-d0c1de27e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Marian Model with self training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5520bd-ed7f-42e4-b465-aca2c40b2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "import copy\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your data (Assuming the CSV is in the same directory)\n",
    "# df = pd.read_csv(\"pmindia_en_ta_39k.csv\") \n",
    "\n",
    "# ==========================================\n",
    "# 1. Load Initial Model & Tokenizer\n",
    "# ==========================================\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"  # Base pre-trained model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Dataset Class for Supervised Learning\n",
    "# ==========================================\n",
    "class EnTaMarianDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.en = df[\"en\"].astype(str).tolist()\n",
    "        self.ta = df[\"ta\"].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.en[idx]\n",
    "        tgt_text = self.ta[idx]\n",
    "\n",
    "        src_enc = self.tokenizer(src_text, max_length=self.max_length, \n",
    "                                 truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tgt_enc = self.tokenizer(tgt_text, max_length=self.max_length, \n",
    "                                     truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = src_enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = src_enc[\"attention_mask\"].squeeze(0)\n",
    "        labels = tgt_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# Initializing Dataset and Dataloader\n",
    "dataset = EnTaMarianDataset(df, tokenizer, max_length=128)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Phase 1: Supervised Fine-Tuning (SFT)\n",
    "# ==========================================\n",
    "print(\"Starting Phase 1: Supervised Fine-Tuning...\")\n",
    "num_epochs = 1\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    # for batch in train_loader: ... (Standard loop you provided)\n",
    "    # [Insert your training loop logic here]\n",
    "\n",
    "print(\"Fine-tuning complete. Saving Phase 1 model...\")\n",
    "model.save_pretrained(\"marian_enhi_to_entamil_finetuned\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Phase 2: Algorithm 1 (Self-Training)\n",
    "# ==========================================\n",
    "\n",
    "def run_self_training_adaptation(model, tokenizer, document_sentences, \n",
    "                                 passes=2, steps=3, alpha=1e-5, lam=0.01):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1 from Mansimov et al. (2021).\n",
    "    - theta_hat: The weights from Phase 1 (Original Weights).\n",
    "    - l_pseudo: Cross-entropy using model's own predictions as targets.\n",
    "    - lam: The decay prior strength (regularization).\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Phase 2: Document-Level Self-Training Adaptation...\")\n",
    "    \n",
    "    # 1. Store the original weights (theta_hat) as a reference point\n",
    "    # We clone them to ensure they stay fixed while 'model' updates.\n",
    "    theta_hat = {n: p.clone().detach().to(device) for n, p in model.named_parameters()}\n",
    "    \n",
    "    # Process the document\n",
    "    for p in range(1, passes + 1):\n",
    "        print(f\"Pass {p}/{passes}\")\n",
    "        \n",
    "        for i, sentence in enumerate(document_sentences):\n",
    "            # STEP A: Generate Pseudo-labels (Y_hat_i)\n",
    "            model.eval()\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # We use generate to get the model's best current guess\n",
    "                pseudo_labels = model.generate(**inputs)\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            # STEP B: Inner adaptation steps (j)\n",
    "            for j in range(steps):\n",
    "                # Forward pass using the model's own guess as the 'true' labels\n",
    "                outputs = model(**inputs, labels=pseudo_labels)\n",
    "                l_pseudo = outputs.loss\n",
    "                \n",
    "                # Backprop to find gradients\n",
    "                model.zero_grad()\n",
    "                l_pseudo.backward()\n",
    "                \n",
    "                # STEP C: Custom Weight Update with Decay Prior\n",
    "                # Equation: θ ← θ - α∇θLpseudo + λ(θ_hat - θ)\n",
    "                with torch.no_grad():\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            # Gradient descent part\n",
    "                            grad_update = alpha * param.grad\n",
    "                            # Decay prior part (rubber band back to theta_hat)\n",
    "                            decay_pull = lam * (theta_hat[name] - param)\n",
    "                            \n",
    "                            # Apply combined update\n",
    "                            param.data = param.data - grad_update + decay_pull\n",
    "                \n",
    "                model.zero_grad()\n",
    "\n",
    "# ==========================================\n",
    "# 5. Execution of Self-Training\n",
    "# ==========================================\n",
    "\n",
    "# Example: A 'Document' is a list of contextually related sentences.\n",
    "target_document = [\n",
    "    \"Digital transformation is essential for modern business.\",\n",
    "    \"It involves integrating technology into all areas of operation.\",\n",
    "    \"This shift changes how companies deliver value to customers.\"\n",
    "]\n",
    "\n",
    "# Run the algorithm\n",
    "run_self_training_adaptation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    document_sentences=target_document,\n",
    "    passes=2,   # p\n",
    "    steps=3,    # j\n",
    "    alpha=1e-5, # learning rate α\n",
    "    lam=0.01    # decay prior λ\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 6. Final Save\n",
    "# ==========================================\n",
    "print(\"Saving Final Self-Trained Model...\")\n",
    "model.save_pretrained(\"marian_final_adapted_document_model\")\n",
    "tokenizer.save_pretrained(\"marian_final_adapted_document_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1901cdb-fa9a-48c0-b374-f391f8564de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d62c78-125b-46f2-b381-a9404c282825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD MODEL & TOKENIZER\n",
    "# ==========================================\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\" \n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATASET CLASS\n",
    "# ==========================================\n",
    "class EnTaMarianDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.en = df[\"en\"].astype(str).tolist()\n",
    "        self.ta = df[\"ta\"].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.en[idx]\n",
    "        tgt_text = self.ta[idx]\n",
    "\n",
    "        src_enc = self.tokenizer(src_text, max_length=self.max_length, \n",
    "                                 truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tgt_enc = self.tokenizer(tgt_text, max_length=self.max_length, \n",
    "                                     truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        labels = tgt_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": src_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 3. HYBRID LOSS & FREEZING LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def prepare_model_for_hybrid_training(model):\n",
    "    \"\"\"\n",
    "    Freezes all layers except the last two decoder layers \n",
    "    to preserve core linguistic knowledge.\n",
    "    \"\"\"\n",
    "    # Freeze everything first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last two decoder layers\n",
    "    # MarianMT: model.model.decoder.layers\n",
    "    num_decoder_layers = len(model.model.decoder.layers)\n",
    "    for i in range(num_decoder_layers - 2, num_decoder_layers):\n",
    "        for param in model.model.decoder.layers[i].parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    # Unfreeze the final output head\n",
    "    for param in model.lm_head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_hybrid_loss(model, batch, alpha=0.5):\n",
    "    \"\"\"\n",
    "    L_hybrid = alpha * L_pseudo + (1 - alpha) * L_gold\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    gold_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    # --- Part A: Gold Loss ---\n",
    "    outputs_gold = model(input_ids=input_ids, attention_mask=attention_mask, labels=gold_labels)\n",
    "    l_gold = outputs_gold.loss\n",
    "\n",
    "    # --- Part B: Pseudo Loss ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate model's own translation as target\n",
    "        pseudo_labels = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    model.train()\n",
    "\n",
    "    # Calculate loss against the pseudo-labels\n",
    "    outputs_pseudo = model(input_ids=input_ids, attention_mask=attention_mask, labels=pseudo_labels)\n",
    "    l_pseudo = outputs_pseudo.loss\n",
    "\n",
    "    # --- Weighted Sum ---\n",
    "    return (alpha * l_pseudo) + ((1 - alpha) * l_gold)\n",
    "\n",
    "# ==========================================\n",
    "# 4. FULL EXECUTION SCRIPT\n",
    "# ==========================================\n",
    "\n",
    "# Load your dataframe\n",
    "# df = pd.read_csv(\"pmindia_en_ta_39k.csv\")\n",
    "# dataset = EnTaMarianDataset(df, tokenizer)\n",
    "# train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# A. Prepare model (Freezing)\n",
    "model = prepare_model_for_hybrid_training(model)\n",
    "\n",
    "# B. Setup Optimizer (only for parameters that require grad)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
    "\n",
    "# C. Hybrid Training Loop\n",
    "num_epochs = 1\n",
    "alpha_param = 0.4 # Weight for pseudo-labels\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Hybrid Loss Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute Hybrid Loss\n",
    "        loss = calculate_hybrid_loss(model, batch, alpha=alpha_param)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Hybrid Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. SAVE FINAL MODEL\n",
    "# ==========================================\n",
    "model.save_pretrained(\"marian_hybrid_loss_model\")\n",
    "tokenizer.save_pretrained(\"marian_hybrid_loss_model\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb53863-5c57-4cc1-b262-db63c8e17b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc535aa3-d313-4985-9dd2-d4de98485654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    MarianMTModel, \n",
    "    MarianTokenizer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# 1. Setup Device & Base Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"  # The base MMT model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2. Load & Prepare the PMIndia Dataset\n",
    "# In the paper, they used 1277 aligned documents\n",
    "# We assume your CSV 'pmindia_curated.csv' has columns: 'en' and 'hi'\n",
    "df = pd.read_csv(\"pmindia_curated.csv\")\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"en\"]]\n",
    "    targets = [ex for ex in examples[\"hi\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the whole dataset\n",
    "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 3. Define Training Arguments (SFT Phase)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./marian_mmt_ft_results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=3e-5,       # Standard for fine-tuning\n",
    "    per_device_train_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,       # Number of passes over the 1277 docs\n",
    "    predict_with_generate=True,\n",
    "    fp16=True if torch.cuda.is_available() else False, # Speed up if GPU\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 4. Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. Start Supervised Fine-Tuning\n",
    "print(\"Starting SFT (MMT + FT) phase...\")\n",
    "trainer.train()\n",
    "\n",
    "# 6. Save the 'Robust Base'\n",
    "# This model will now be used for Phase 4.3.2 and 4.3.3\n",
    "model.save_pretrained(\"./marian_pmindia_base_ft\")\n",
    "tokenizer.save_pretrained(\"./marian_pmindia_base_ft\")\n",
    "print(\"Phase 4.3.4 Complete. Base model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35bf1d5a-9875-4a81-b5dc-7fbe97cd1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f31bc-340d-46ff-af33-863d063ba1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    MarianMTModel, \n",
    "    MarianTokenizer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 1. Load the \"Robust Base\" from Phase 4.3.4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"./marian_pmindia_base_ft\" # The model saved in the previous step\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "# 2. Configure LoRA (Low-Rank Adaptation)\n",
    "# r: the rank (bottleneck size). lora_alpha: scaling factor.\n",
    "# target_modules: In MarianMT, these are usually the attention layers.\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Adapting the Attention Query and Value\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# 3. Apply LoRA to the model\n",
    "# This freezes the base weights and adds the trainable adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters to verify (usually <1% of total)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# \n",
    "\n",
    "# 4. Prepare Dataset (Same as previous steps)\n",
    "df = pd.read_csv(\"pmindia_curated.csv\")\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"en\"]]\n",
    "    targets = [ex for ex in examples[\"hi\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 5. Define Training Arguments\n",
    "# Note: LoRA usually requires a slightly higher learning rate than full FT\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./marian_lora_results\",\n",
    "    learning_rate=1e-4, \n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 6. Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 7. Start LoRA Training\n",
    "print(\"Starting LoRA Fine-Tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# 8. Save the LoRA Adapters\n",
    "# Note: This only saves the tiny adapter weights, not the whole model\n",
    "model.save_pretrained(\"./marian_lora_adapters\")\n",
    "print(\"LoRA Fine-Tuning Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256cdab9-4334-40ea-80d5-8b534af792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#span based context injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0993d-8a75-4d86-964d-5e7b4216d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD MODEL & ADD SPECIAL TOKENS\n",
    "# ==========================================\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\" \n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define the span tokens\n",
    "special_tokens_dict = {'additional_special_tokens': ['<prev>', '</prev>', '<curr>', '</curr>', '<next>', '</next>']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Resize the model embeddings to account for new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONTEXTUAL DATASET CLASS\n",
    "# ==========================================\n",
    "class SpanContextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        \"\"\"\n",
    "        Expects a DataFrame where rows are in chronological order \n",
    "        belonging to the same document.\n",
    "        \"\"\"\n",
    "        self.en_sentences = df[\"en\"].astype(str).tolist()\n",
    "        self.hi_sentences = df[\"hi\"].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Identify neighbors for context injection\n",
    "        prev_s = self.en_sentences[idx-1] if idx > 0 else \"\"\n",
    "        curr_s = self.en_sentences[idx]\n",
    "        next_s = self.en_sentences[idx+1] if idx < len(self.en_sentences)-1 else \"\"\n",
    "\n",
    "        # Format input: <prev> Si-1 </prev> <curr> Si </curr> <next> Si+1 </next>\n",
    "        contextual_input = (\n",
    "            f\"<prev> {prev_s} </prev> \"\n",
    "            f\"<curr> {curr_s} </curr> \"\n",
    "            f\"<next> {next_s} </next>\"\n",
    "        )\n",
    "        \n",
    "        # Target remains just the current sentence (Si) in Hindi\n",
    "        target_text = self.hi_sentences[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        src_enc = self.tokenizer(contextual_input, max_length=self.max_length, \n",
    "                                 truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            tgt_enc = self.tokenizer(target_text, max_length=128, \n",
    "                                     truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        labels = tgt_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": src_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 3. HYBRID LOSS WITH SPAN CONTEXT\n",
    "# ==========================================\n",
    "def calculate_hybrid_span_loss(model, batch, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Combines the context-augmented input with the Hybrid Loss (αLpseudo + (1-α)Lgold)\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    gold_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    # 1. Gold Loss (L_gold)\n",
    "    outputs_gold = model(input_ids=input_ids, attention_mask=attention_mask, labels=gold_labels)\n",
    "    l_gold = outputs_gold.loss\n",
    "\n",
    "    # 2. Pseudo Loss (L_pseudo) using Context\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pseudo_labels = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    model.train()\n",
    "    \n",
    "    outputs_pseudo = model(input_ids=input_ids, attention_mask=attention_mask, labels=pseudo_labels)\n",
    "    l_pseudo = outputs_pseudo.loss\n",
    "\n",
    "    return (alpha * l_pseudo) + ((1 - alpha) * l_gold)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING EXECUTION\n",
    "# ==========================================\n",
    "# dataset = SpanContextDataset(df, tokenizer)\n",
    "# loader = DataLoader(dataset, batch_size=8, shuffle=False) # Shuffle=False to keep doc order\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "print(\"Training MMT + FT + HL + Span...\")\n",
    "model.train()\n",
    "# for batch in loader:\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = calculate_hybrid_span_loss(model, batch)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# Save final model\n",
    "# model.save_pretrained(\"marian_span_context_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0035de-38d9-48f4-83f4-1d9d82d0cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d74d13-d54d-488d-b13a-b7196f925113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from bert_score import BERTScorer\n",
    "from transformers import MarianMTModel, MarianTokenizer, AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD MODEL & BERT SCORER\n",
    "# ==========================================\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Initialize BERTScorer for Semantic Loss (L_bert)\n",
    "# Using 'bert-base-multilingual-cased' for English-Hindi/Tamil support\n",
    "scorer = BERTScorer(lang=\"hi\", device=device, rescale_with_baseline=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. HYBRID + BERTSCORE LOSS FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def compute_mmt_plus_all_loss(model, batch, alpha=0.4, lam_bert=0.3):\n",
    "    \"\"\"\n",
    "    Implements Phase 4.3.7: MMT + FT + HL + Span + BERTScore-Loss\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    gold_labels = batch[\"labels\"].to(device) # Shape: [batch, seq_len]\n",
    "\n",
    "    # --- 1. L_gold (Supervised Cross-Entropy) ---\n",
    "    outputs_gold = model(input_ids=input_ids, attention_mask=attention_mask, labels=gold_labels)\n",
    "    l_gold = outputs_gold.loss\n",
    "\n",
    "    # --- 2. L_pseudo (Self-Training Cross-Entropy) ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate model's own current best prediction\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    model.train()\n",
    "    \n",
    "    # We must pad generated_ids to match gold_labels length or vice versa for the loss\n",
    "    outputs_pseudo = model(input_ids=input_ids, attention_mask=attention_mask, labels=generated_ids)\n",
    "    l_pseudo = outputs_pseudo.loss\n",
    "\n",
    "    # --- 3. L_hybrid (Weighted Cross-Entropy) ---\n",
    "    l_hybrid = (alpha * l_pseudo) + ((1 - alpha) * l_gold)\n",
    "\n",
    "    # --- 4. L_bert (Semantic Loss) ---\n",
    "    # Convert token IDs back to strings for BERTScore calculation\n",
    "    # We use the 'generated_ids' vs 'gold_labels'\n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up gold labels (-100 to pad_id) before decoding\n",
    "    clean_gold = gold_labels.clone()\n",
    "    clean_gold[clean_gold == -100] = tokenizer.pad_token_id\n",
    "    decoded_refs = tokenizer.batch_decode(clean_gold, skip_special_tokens=True)\n",
    "\n",
    "    # BERTScore returns (P, R, F1). We use 1 - F1 as the semantic distance.\n",
    "    _, _, f1 = scorer.score(decoded_preds, decoded_refs)\n",
    "    l_bert = 1 - f1.mean()\n",
    "\n",
    "    # --- 5. Total Loss Calculation ---\n",
    "    l_total = ((1 - lam_bert) * l_hybrid) + (lam_bert * l_bert)\n",
    "    \n",
    "    return l_total\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING STEP EXAMPLE\n",
    "# ==========================================\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Example batch processing\n",
    "# for batch in dataloader:\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = compute_mmt_plus_all_loss(model, batch, alpha=0.4, lam_bert=0.2)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe100c-768d-4dac-b379-5c765d9e7610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774aa84e-b80a-46fc-a422-aa5a8cbb1c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
