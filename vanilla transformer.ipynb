{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c793cb2-654a-4540-b486-4c4a55d64016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 10577\n",
      "{'en': 'Prime Minister Narendra Modi met Her Majesty Queen Maxima of the Kingdom of the Netherlands today.', 'ta': 'சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Paths to your files (edit this to your folder)\n",
    "# data_dir = Path(\"/../en-ta.txt/\")\n",
    "en_path = \"/Users/niteeshkumar/Documents/document_level_nmt/en-ta.txt/pmindia.en-ta.en\"\n",
    "ta_path = \"/Users/niteeshkumar/Documents/document_level_nmt/en-ta.txt/pmindia.en-ta.ta\"\n",
    "\n",
    "# 1. Read files\n",
    "with open(en_path, \"r\", encoding=\"utf-8\") as f_en:\n",
    "    en_lines = [line.strip() for line in f_en]\n",
    "\n",
    "with open(ta_path, \"r\", encoding=\"utf-8\") as f_ta:\n",
    "    ta_lines = [line.strip() for line in f_ta]\n",
    "\n",
    "assert len(en_lines) == len(ta_lines), \"Line counts do not match!\"\n",
    "\n",
    "# 2. Build parallel sentence pairs\n",
    "parallel_pairs = [\n",
    "    {\"en\": src, \"ta\": tgt}\n",
    "    for src, tgt in zip(en_lines, ta_lines)\n",
    "    if src and tgt  # drop empty lines\n",
    "]\n",
    "\n",
    "print(f\"Total pairs: {len(parallel_pairs)}\")\n",
    "print(parallel_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255a8abc-5c93-491d-a535-a05e336e67a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The rationalization of Autonomous Bodies under Department of Health & Family Welfare will involve inter-ministerial consultations and review of existing bye laws of these bodies.\\tசுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.\\n'\n",
      "'The time frame for implementation is one year,\\tஇதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.\\n'\n",
      "'Since the DoHFW provides funds to the hospitals, the grants can be given from the Department to the hospital directly.\\tமருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் குடும்ப நலத் துறை வழங்கி வருவதால், நிதியுதவியையும் மருத்துமனைகளுக்கு துறையே நேரில் வழங்க முடியும்.\\n'\n",
      "'RAN functions can, therefore, be vested in DoHFW.\\tதேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சுகாதாரம் மற்றும் குடும்பநலத் துறையிடம் இருக்கும்.\\n'\n",
      "'Managing Committee of RAN Society will meet to dissolve the Autonomous Body (AB) as per provisions of Societies Registration Act, 1860 (SRA).\\tசங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறைகளின்படி, தன்னாட்சி அமைப்பை கலைக்க, தேசிய ஆரோக்கிய நிதி சங்கத்தின் நிர்வாகிகள் குழு கூடும்.\\n'\n",
      "'In addition to this, Health Minister’s Cancer Patient Fund (HMCPF) shall also be transferred to the Department.\\tஇதற்கு மேலாக, சுகாதார அமைச்சரின் புற்றுநோயால் பாதிக்கப்பட்ட நோயாளிகளுக்கான நிதி-யும், துறைக்கே மாற்றப்படுகிறது.\\n'\n",
      "'The timeline required for this is one year.\\tஇதற்கு ஓராண்டு கால அவகாசம் தேவைப்படுகிறது.\\n'\n",
      "'Jansankhya Sthirata Kosh (JSK) was set up with a corpus grant of Rs.100 crores in the year 2003 to raise awareness for population stabilization strategies.\\tமக்கள் தொகையை நிலைப்படுத்துவதற்கான உத்திகள் குறித்து விழிப்புணர்வை ஏற்படுத்துவதற்காக, ரூ.100 கோடி நிதியுடன், கடந்த 2003-ம் ஆண்டில் தேசிய மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைக்கப்பட்டது.\\n'\n",
      "'JSK organizes various activities with target populations as a part of its mandate.\\tதேசிய மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைப்பு, தனது நோக்கத்தின் ஒரு பகுதியாக, மக்கள் தொகையை இலக்காகக் கொண்டு பல்வேறு நிகழ்ச்சிகளுக்கு ஏற்பாடு செய்தது.\\n'\n",
      "'There has been no continuous funding to JSK from the Ministry.\\tஅமைச்சகத்திடமிருந்து மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைப்புக்கு தொடர்ந்து நிதி வழங்கப்படவில்லை.\\n'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tsv_path = \"/Users/niteeshkumar/Documents/document_level_nmt/pmindia.v1.ta-en.tsv\" # update this\n",
    "\n",
    "with open(tsv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(repr(line))\n",
    "        if i == 9:   # first 10 lines\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663e224c-4a6c-444c-a477-979fc5c5aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 39526\n",
      "{'en': 'The rationalization of Autonomous Bodies under Department of Health & Family Welfare will involve inter-ministerial consultations and review of existing bye laws of these bodies.', 'ta': 'சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.'}\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "with open(tsv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 2:\n",
    "            continue  # skip any abnormal lines\n",
    "\n",
    "        en, ta = parts\n",
    "        en, ta = en.strip(), ta.strip()\n",
    "        if en and ta:\n",
    "            pairs.append({\"en\": en, \"ta\": ta})\n",
    "\n",
    "print(\"Total pairs:\", len(pairs))\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c14eb3f-9c28-449c-a783-8fbbd9e993a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9443669-4598-4c32-b35f-7f1196a914dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch\n",
    "\n",
    "# model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "# batch_size = 64\n",
    "# aligned_pairs = []\n",
    "\n",
    "# for start in range(0, len(pairs), batch_size):\n",
    "#     end = start + batch_size\n",
    "#     en_batch = [p[\"en\"] for p in pairs[start:end]]\n",
    "#     ta_batch = [p[\"ta\"] for p in pairs[start:end]]\n",
    "\n",
    "#     en_emb = model.encode(en_batch, convert_to_tensor=True)\n",
    "#     ta_emb = model.encode(ta_batch, convert_to_tensor=True)\n",
    "\n",
    "#     cos_scores = util.cos_sim(en_emb, ta_emb).diagonal()  # only batch diag\n",
    "\n",
    "#     for pair, score in zip(pairs[start:end], cos_scores):\n",
    "#         if score.item() >= 0.7:\n",
    "#             aligned_pairs.append({\n",
    "#                 \"en\": pair[\"en\"],\n",
    "#                 \"ta\": pair[\"ta\"],\n",
    "#                 \"score\": float(score),\n",
    "#             })\n",
    "\n",
    "# print(\"Original:\", len(pairs), \"Aligned:\", len(aligned_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f32eb2-5b05-45d5-bd6d-f5b9152d2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape(aligned_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea53eb79-d49f-4d60-b929-09f19ce7d62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 4941\n",
      "First doc len: 8\n"
     ]
    }
   ],
   "source": [
    "DOC_LEN = 8  # sentences per pseudo-document\n",
    "\n",
    "documents = []\n",
    "for i in range(0, len(pairs), DOC_LEN):\n",
    "    chunk = pairs[i:i+DOC_LEN]\n",
    "    if len(chunk) < 2:\n",
    "        break\n",
    "    documents.append(chunk)\n",
    "\n",
    "print(\"Num documents:\", len(documents))\n",
    "print(\"First doc len:\", len(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41405122-a37e-4259-b268-438fcc4f7537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'The rationalization of Autonomous Bodies under Department of Health & Family Welfare will involve inter-ministerial consultations and review of existing bye laws of these bodies.',\n",
       "  'ta': 'சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள்ள தன்னாட்சி அமைப்புகளை சீரமைக்கும் நடவடிக்கையில், அமைச்சகங்களுக்கு இடையேயான ஆலோசனைகள் நடைபெறும் மற்றும் இந்த அமைப்புகளுக்கான ஏற்கனவே உள்ள சட்டதிட்டங்கள் மறுஆய்வு செய்யப்படும்.'},\n",
       " {'en': 'The time frame for implementation is one year,',\n",
       "  'ta': 'இதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.'},\n",
       " {'en': 'Since the DoHFW provides funds to the hospitals, the grants can be given from the Department to the hospital directly.',\n",
       "  'ta': 'மருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் குடும்ப நலத் துறை வழங்கி வருவதால், நிதியுதவியையும் மருத்துமனைகளுக்கு துறையே நேரில் வழங்க முடியும்.'},\n",
       " {'en': 'RAN functions can, therefore, be vested in DoHFW.',\n",
       "  'ta': 'தேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சுகாதாரம் மற்றும் குடும்பநலத் துறையிடம் இருக்கும்.'},\n",
       " {'en': 'Managing Committee of RAN Society will meet to dissolve the Autonomous Body (AB) as per provisions of Societies Registration Act, 1860 (SRA).',\n",
       "  'ta': 'சங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறைகளின்படி, தன்னாட்சி அமைப்பை கலைக்க, தேசிய ஆரோக்கிய நிதி சங்கத்தின் நிர்வாகிகள் குழு கூடும்.'},\n",
       " {'en': 'In addition to this, Health Minister’s Cancer Patient Fund (HMCPF) shall also be transferred to the Department.',\n",
       "  'ta': 'இதற்கு மேலாக, சுகாதார அமைச்சரின் புற்றுநோயால் பாதிக்கப்பட்ட நோயாளிகளுக்கான நிதி-யும், துறைக்கே மாற்றப்படுகிறது.'},\n",
       " {'en': 'The timeline required for this is one year.',\n",
       "  'ta': 'இதற்கு ஓராண்டு கால அவகாசம் தேவைப்படுகிறது.'},\n",
       " {'en': 'Jansankhya Sthirata Kosh (JSK) was set up with a corpus grant of Rs.100 crores in the year 2003 to raise awareness for population stabilization strategies.',\n",
       "  'ta': 'மக்கள் தொகையை நிலைப்படுத்துவதற்கான உத்திகள் குறித்து விழிப்புணர்வை ஏற்படுத்துவதற்காக, ரூ.100 கோடி நிதியுடன், கடந்த 2003-ம் ஆண்டில் தேசிய மக்கள் தொகை நிலைப்படுத்துதல் நிதி அமைக்கப்பட்டது.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e943d5f7-2c03-42a7-b971-4326b3c65909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39526"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c9b927b-3d8b-41cd-be8a-b4d8046d5e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The rationalization of Autonomous Bodies under...</td>\n",
       "      <td>சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The time frame for implementation is one year,</td>\n",
       "      <td>இதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>மருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் க...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAN functions can, therefore, be vested in DoHFW.</td>\n",
       "      <td>தேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சு...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>சங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறை...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>We will also get to see its direct benefit dur...</td>\n",
       "      <td>இரு நாடுகளையும் சேர்ந்த தொழில் துறையினருடன் இன...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39522</th>\n",
       "      <td>I am confident that our dialogue and the agree...</td>\n",
       "      <td>இன்றைய நமது பேச்சுவார்த்தைகள் மற்றும் கையெழுத்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39523</th>\n",
       "      <td>With these words, I once again warmly welcome ...</td>\n",
       "      <td>இந்தக் கருத்துகளுடன், பிரதமர் அபே-வையும், அவரு...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39524</th>\n",
       "      <td>Ijyo De Gozaimas (That’s all for now)</td>\n",
       "      <td>Ijyo De Gozaimas (தற்போது இவ்வளவு தான்)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39525</th>\n",
       "      <td>Arigato Gozaimas (Thank you)</td>\n",
       "      <td>Arigato Gozaimas (உங்களுக்கு நன்றி.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39526 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      The rationalization of Autonomous Bodies under...   \n",
       "1         The time frame for implementation is one year,   \n",
       "2      Since the DoHFW provides funds to the hospital...   \n",
       "3      RAN functions can, therefore, be vested in DoHFW.   \n",
       "4      Managing Committee of RAN Society will meet to...   \n",
       "...                                                  ...   \n",
       "39521  We will also get to see its direct benefit dur...   \n",
       "39522  I am confident that our dialogue and the agree...   \n",
       "39523  With these words, I once again warmly welcome ...   \n",
       "39524              Ijyo De Gozaimas (That’s all for now)   \n",
       "39525                       Arigato Gozaimas (Thank you)   \n",
       "\n",
       "                                                      ta  \n",
       "0      சுகாதாரம் மற்றும் குடும்பநலத் துறையின் கீழ் உள...  \n",
       "1            இதனை செயல்படுத்துவதற்கான காலக்கெடு ஓராண்டு.  \n",
       "2      மருத்துவமனைகளுக்கான நிதியை சுகாதாரம் மற்றும் க...  \n",
       "3      தேசிய ஆரோக்கிய நிதி அமைப்பின் செயல்பாடுகள், சு...  \n",
       "4      சங்கங்கள் பதிவுச் சட்டம், 1860-ல் உள்ள வழிமுறை...  \n",
       "...                                                  ...  \n",
       "39521  இரு நாடுகளையும் சேர்ந்த தொழில் துறையினருடன் இன...  \n",
       "39522  இன்றைய நமது பேச்சுவார்த்தைகள் மற்றும் கையெழுத்...  \n",
       "39523  இந்தக் கருத்துகளுடன், பிரதமர் அபே-வையும், அவரு...  \n",
       "39524            Ijyo De Gozaimas (தற்போது இவ்வளவு தான்)  \n",
       "39525               Arigato Gozaimas (உங்களுக்கு நன்றி.)  \n",
       "\n",
       "[39526 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pairs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef152d8b-4727-4cf4-8f73-b4b5d3f25cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7597e4-475d-48cc-a1c8-97a854a973e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77047b88-16b9-4558-aff2-f6fa306edc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7500f48-6696-41cd-a2b6-3eda1ebd607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import math\n",
    "# import json\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "#                            (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66193af9-74d4-44c9-b654-26765e23196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- Positional Encoding ----------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)          # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)                        # [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# ---------- Multi-Head Attention ----------\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [batch, seq_len, d_model]\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # [batch, seq, d_model]\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        # Split into heads: [batch, heads, seq, d_k]\n",
    "        def split_heads(x):\n",
    "            return x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # scores: [batch, heads, seq_q, seq_k]\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: [batch, 1, 1, seq_k] or [batch, 1, seq_q, seq_k]\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, V)  # [batch, heads, seq_q, d_k]\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )  # [batch, seq_q, d_model]\n",
    "\n",
    "        out = self.W_o(context)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---------- Position-wise Feed Forward ----------\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "# ---------- Encoder / Decoder Layers ----------\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        # Self-attention\n",
    "        attn_out = self.self_attn(x, x, x, src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        # Feed-forward\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        # Masked self-attention (decoder)\n",
    "        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_out))\n",
    "        # Encoder–decoder attention\n",
    "        cross_attn_out = self.cross_attn(x, memory, memory, memory_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_out))\n",
    "        # Feed-forward\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------- Encoder / Decoder Stacks ----------\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # src: [batch, src_len]\n",
    "        x = self.tok_embed(src)   # [batch, src_len, d_model]\n",
    "        x = self.pos_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # tgt: [batch, tgt_len]\n",
    "        x = self.tok_embed(tgt)\n",
    "        x = self.pos_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ---------- Full Transformer ----------\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    # standard causal mask for decoder\n",
    "    mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "    return ~mask  # True where allowed, False where masked\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        d_model=512,\n",
    "        N=6,\n",
    "        n_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        max_len=5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "\n",
    "    def make_src_mask(self, src, pad_idx):\n",
    "        # src: [batch, src_len]\n",
    "        mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch, 1, 1, src_len]\n",
    "        return mask\n",
    "\n",
    "    def make_tgt_mask(self, tgt, pad_idx):\n",
    "        # tgt: [batch, tgt_len]\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len]\n",
    "        subseq_mask = generate_square_subsequent_mask(tgt_len, tgt.device)  # [tgt_len, tgt_len]\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, tgt_len, tgt_len]\n",
    "        mask = pad_mask & subseq_mask\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "        src_mask = self.make_src_mask(src, src_pad_idx)\n",
    "        tgt_mask = self.make_tgt_mask(tgt, tgt_pad_idx)\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        out = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Pseudo-code: adapt to your actual file format.\n",
    "def parse_line(line):\n",
    "    # Example input:\n",
    "    # \"enta0The rationalization ...சுகாதாரம் ...1\"\n",
    "    # You likely need a known separator; if not, pre-clean in a script.\n",
    "    # For illustration, assume tab-separated: id \\t en \\t ta \\t idx\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    en = parts[1]\n",
    "    ta = parts[2]\n",
    "    return en, ta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67e93b81-226d-4014-92c5-e4fb509e9836",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# After you build vocabularies:\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m src_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(src_vocab)\n\u001b[1;32m      5\u001b[0m tgt_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tgt_vocab)\n\u001b[1;32m      6\u001b[0m pad_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# make sure 0 is PAD in both vocabs\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# After you build vocabularies:\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "pad_idx = 0  # make sure 0 is PAD in both vocabs\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    N=6,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=256,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    ")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        src, tgt = batch[\"src\"].to(device), batch[\"tgt\"].to(device)\n",
    "        # Teacher forcing: decoder input is tgt[:, :-1], target labels are tgt[:, 1:]\n",
    "        tgt_inp = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, tgt_inp, pad_idx, pad_idx)  # [batch, tgt_len-1, vocab]\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, tgt_vocab_size),\n",
    "            tgt_out.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss = {total_loss / len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06855ad-f8e4-4c00-bac5-8bdc836b28e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251a881-e648-4c9e-aea2-983b9863cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vanila transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84701d6-09b1-4865-b04f-72d01c6d5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1. Vocab building & tokenization\n",
    "# ===============================\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    # Replace with a better tokenizer if you want.\n",
    "    return text.strip().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(simple_tokenize(str(t)))\n",
    "    itos = list(special_tokens)\n",
    "    for tok, freq in counter.items():\n",
    "        if freq >= min_freq and tok not in itos:\n",
    "            itos.append(tok)\n",
    "    stoi = {tok: idx for idx, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "# Build vocabularies from dataframe\n",
    "src_vocab, src_itos = build_vocab(df[\"en\"].tolist(), min_freq=1)\n",
    "tgt_vocab, tgt_itos = build_vocab(df[\"ta\"].tolist(), min_freq=1)\n",
    "\n",
    "pad_idx = src_vocab[PAD_TOKEN]\n",
    "assert pad_idx == tgt_vocab[PAD_TOKEN]\n",
    "\n",
    "def encode_sentence(text, vocab, max_len=256):\n",
    "    tokens = [SOS_TOKEN] + simple_tokenize(str(text)) + [EOS_TOKEN]\n",
    "    ids = [vocab.get(tok, vocab[UNK_TOKEN]) for tok in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[PAD_TOKEN]] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "src_seqs = [encode_sentence(t, src_vocab, max_len=max_len) for t in df[\"en\"]]\n",
    "tgt_seqs = [encode_sentence(t, tgt_vocab, max_len=max_len) for t in df[\"ta\"]]\n",
    "\n",
    "src_tensor = torch.tensor(src_seqs, dtype=torch.long)\n",
    "tgt_tensor = torch.tensor(tgt_seqs, dtype=torch.long)\n",
    "\n",
    "# ===============================\n",
    "# 2. Dataset & DataLoader\n",
    "# ===============================\n",
    "\n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, src_tensor, tgt_tensor):\n",
    "        self.src = src_tensor\n",
    "        self.tgt = tgt_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.src.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": self.src[idx],\n",
    "            \"tgt\": self.tgt[idx],\n",
    "        }\n",
    "\n",
    "dataset = NMTDataset(src_tensor, tgt_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 3. Transformer components\n",
    "# ===============================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)          # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)                        # [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [batch, seq_len, d_model]\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        def split_heads(x):\n",
    "            return x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        Q = split_heads(Q)\n",
    "        K = split_heads(K)\n",
    "        V = split_heads(V)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # scores: [batch, heads, seq_q, seq_k]\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: [batch, 1, 1, seq_k] or [batch, 1, seq_q, seq_k]\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, V)  # [batch, heads, seq_q, d_k]\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )  # [batch, seq_q, d_model]\n",
    "\n",
    "        out = self.W_o(context)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        attn_out = self.self_attn(x, x, x, src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_out))\n",
    "        cross_attn_out = self.cross_attn(x, memory, memory, memory_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        x = self.tok_embed(src)   # [batch, src_len, d_model]\n",
    "        x = self.pos_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads, d_ff, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(N)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        x = self.tok_embed(tgt)\n",
    "        x = self.pos_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "    return ~mask  # True where allowed, False where masked\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        d_model=512,\n",
    "        N=6,\n",
    "        n_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        max_len=5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, N, n_heads, d_ff, dropout, max_len)\n",
    "\n",
    "    def make_src_mask(self, src, pad_idx):\n",
    "        mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return mask\n",
    "\n",
    "    def make_tgt_mask(self, tgt, pad_idx):\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len]\n",
    "        subseq_mask = generate_square_subsequent_mask(tgt_len, tgt.device)  # [tgt_len, tgt_len]\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, tgt_len, tgt_len]\n",
    "        mask = pad_mask & subseq_mask\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "        src_mask = self.make_src_mask(src, src_pad_idx)\n",
    "        tgt_mask = self.make_tgt_mask(tgt, tgt_pad_idx)\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        out = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
    "        return out\n",
    "\n",
    "# ===============================\n",
    "# 4. Training setup\n",
    "# ===============================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    N=6,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=max_len,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5. Training loop\n",
    "# ===============================\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        src = batch[\"src\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "\n",
    "        # Teacher forcing: decoder input is tgt[:, :-1], labels are tgt[:, 1:]\n",
    "        tgt_inp = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        logits = model(src, tgt_inp, pad_idx, pad_idx)  # [batch, tgt_len-1, vocab]\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, tgt_vocab_size),\n",
    "            tgt_out.reshape(-1),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c3c4c-03f4-4a75-befe-01825795023f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
